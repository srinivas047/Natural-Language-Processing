{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Eal4MKZac05_"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from collections import Counter\n","#library imports\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import re\n","import spacy\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import string\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from sklearn.metrics import mean_squared_error\n","import re\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wje4lwp_c69U","outputId":"6510fa15-58fc-4a8e-fabb-014409784502","executionInfo":{"status":"ok","timestamp":1652488724567,"user_tz":240,"elapsed":521,"user":{"displayName":"Sai Srinivas","userId":"04487647299595132437"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["100000\n"]}],"source":["import pandas as pd\n","data = pd.read_csv('/content/task_1_information_extraction_train.tsv', sep = '\\t')\n","print(len(data))"]},{"cell_type":"code","source":["data=data[:10000]"],"metadata":{"id":"0qu4froOTCVI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gb5RSmssdKdB"},"outputs":[],"source":["# Removies Null rows and reseting Index\n","data = data[data['NOTES'].notna()]\n","data = data.reset_index(drop=True)\n","# data = data[:20000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YWFsNFRsdN8u"},"outputs":[],"source":["newdata = pd.concat([data['NOTES'], data['INTER1']], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9L3_qn6jdbTi"},"outputs":[],"source":["tok = spacy.load('en')\n","def tokenize (text):\n","    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n","    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n","    nopunct = regex.sub(\" \", text.lower())\n","    return [token.text for token in tok.tokenizer(nopunct)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0yArwWBdgP1"},"outputs":[],"source":["counts = Counter()\n","docs = {}\n","for i in range(len(newdata['NOTES'])):\n","    docs[i] = tokenize(newdata['NOTES'][i])\n","    counts.update(tokenize(newdata['NOTES'][i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W7CKiRVwdjxa"},"outputs":[],"source":["#creating vocabulary\n","vocab2index = {\"\":0, \"UNK\":1}\n","words = [\"\", \"UNK\"]\n","for word in counts:\n","    vocab2index[word] = len(words)\n","    words.append(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQNosHYpdnNO"},"outputs":[],"source":["def encode_sentence(text, vocab2index, N=70):\n","    tokenized = tokenize(text)\n","    encoded = np.zeros(N, dtype=int)\n","    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n","    length = min(N, len(enc1))\n","    encoded[:length] = enc1[:length]\n","    return encoded, length"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_QIMX-Ydsp4","outputId":"6a73b0fa-a1a7-4415-bb48-a3a354db82fb","executionInfo":{"status":"ok","timestamp":1652490279620,"user_tz":240,"elapsed":1856,"user":{"displayName":"Sai Srinivas","userId":"04487647299595132437"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}],"source":["newdata['encoded'] = newdata['NOTES'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7WY2Lvlj6_g"},"outputs":[],"source":["X_train = []\n","for i in range(len(newdata['encoded'])):\n","    X_train.append(newdata['encoded'][i][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTMEchPBi6J6"},"outputs":[],"source":["target = []\n","for i in range(len(newdata['INTER1'])):\n","    target.append(newdata['INTER1'][i])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fDCR5ASlzEW","outputId":"acc42d99-e8e7-4c9e-88d0-f100fbe822e5","executionInfo":{"status":"ok","timestamp":1652490289000,"user_tz":240,"elapsed":148,"user":{"displayName":"Sai Srinivas","userId":"04487647299595132437"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(9696, 70)\n"]}],"source":["X_train = np.array(X_train)\n","print(X_train.shape)\n","target = np.array(target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGEGAlQ_h2bN"},"outputs":[],"source":["\n","# sc = MinMaxScaler(feature_range = (0, 1))\n","# training_set_scaled = sc.fit_transform(X_train)\n","\n","X_train1 = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OISvNUsyf0pO"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqnBJQhsfNC0"},"outputs":[],"source":["# Inter1\n","regressor = Sequential()\n","\n","regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n","regressor.add(Dropout(0.2))\n","\n","regressor.add(LSTM(units = 50, return_sequences = True))\n","regressor.add(Dropout(0.2))\n","\n","regressor.add(LSTM(units = 50, return_sequences = True))\n","regressor.add(Dropout(0.2))\n","\n","regressor.add(LSTM(units = 50))\n","regressor.add(Dropout(0.2))\n","\n","regressor.add(Dense(units = 1))\n","\n","regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n","\n","regressor.fit(X_train1, target, epochs = 500, batch_size = 32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cElDsgaUmvq-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652488856606,"user_tz":240,"elapsed":2490,"user":{"displayName":"Sai Srinivas","userId":"04487647299595132437"}},"outputId":"777156f8-6d1c-41ad-d7f3-d2b38c841d48"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  import sys\n"]}],"source":["testdata = pd.read_csv('/content/task_1_information_extraction_valid.tsv', sep = '\\t')\n","# testdata = testdata\n","\n","# testdata['NOTES'] = testdata['NOTES'].fillna('')\n","testdata = testdata[testdata['NOTES'].notna()]\n","testdata = testdata.reset_index(drop=True)\n","testdata['encoded'] = testdata['NOTES'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyrVB7uXD1hQ"},"outputs":[],"source":["y = []\n","for i in range(len(testdata['INTER1'])):\n","    y.append(testdata['INTER1'][i])\n","\n","y = np.array(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBUksNh4nCXn"},"outputs":[],"source":["X_test = []\n","for i in range(len(testdata['encoded'])):\n","    X_test.append(testdata['encoded'][i][0])\n","X_test = np.array(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjZqAveYnI75"},"outputs":[],"source":["X_test1 = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXIPR-VOngLb"},"outputs":[],"source":["predicted = regressor.predict(X_test1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61EBNkBlDijf"},"outputs":[],"source":["predictions = []\n","for i in predicted:\n","  predictions.append(i.item())"]},{"cell_type":"code","source":["predictions[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pVu5xRaBtTZ","executionInfo":{"status":"ok","timestamp":1652489284756,"user_tz":240,"elapsed":133,"user":{"displayName":"Sai Srinivas","userId":"04487647299595132437"}},"outputId":"92fcb52b-587c-40d7-c854-a3c3047d1a76"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3.3377845287323,\n"," 4.8231120109558105,\n"," 3.646108627319336,\n"," 3.5548553466796875,\n"," 3.4154322147369385,\n"," 3.39959454536438,\n"," 3.6519358158111572,\n"," 3.4529504776000977,\n"," 3.2848219871520996,\n"," 3.4089789390563965]"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["import numpy\n","corr_matrix = numpy.corrcoef(y, predictions)\n","corr = corr_matrix[0,1]\n","R_sq = corr**2*2\n"," \n","print(\"R2 Value for (5, 1)\", R_sq)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9omm3ziqBmFe","executionInfo":{"status":"ok","timestamp":1652489881748,"user_tz":240,"elapsed":135,"user":{"displayName":"Sai Srinivas","userId":"04487647299595132437"}},"outputId":"6425d55c-78d8-4539-95d3-e2f76e46f2fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R2 Value for (5, 1) 0.1791379015126493\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Task1_Inter1_Final.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}