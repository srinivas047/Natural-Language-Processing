{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JOzsUXmO0J5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_kln74OO-sK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers.optimization import  Adafactor \n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "sCqZhhb0nKz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPc3AL0cPMAi"
      },
      "outputs": [],
      "source": [
        "train_df=pd.read_csv('task_2_event_summarization_train.tsv', sep = '\\t')\n",
        "valid_df=pd.read_csv('task_2_event_summarization_valid.tsv', sep = '\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSpr9hVSQEGk"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea7Yf5JMPlcv"
      },
      "outputs": [],
      "source": [
        "len(train_df)\n",
        "len(valid_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "ePSjd2ZVy9Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTGvArzbPuBd"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "num_of_batches = len(train_df)/batch_size\n",
        "# num_of_epochs = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0TxCWJ0P4ly"
      },
      "outputs": [],
      "source": [
        "num_of_batches=int(num_of_batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjEyL26FP60g"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    dev = torch.device(\"cuda:0\") \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    dev = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa5mYNkcP-q4"
      },
      "outputs": [],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-large', return_dict=True)\n",
        "#moving the model to device(GPU/CPU)\n",
        "model.to(dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoR7Eea1Qi3p"
      },
      "outputs": [],
      "source": [
        "optimizer = Adafactor(\n",
        "    model.parameters(),\n",
        "    lr=1e-3,\n",
        "    eps=(1e-30, 1e-3),\n",
        "    clip_threshold=1.0,\n",
        "    decay_rate=-0.8,\n",
        "    beta1=None,\n",
        "    weight_decay=0.0,\n",
        "    relative_step=False,\n",
        "    scale_parameter=False,\n",
        "    warmup_init=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CINQqGSWQlfW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def progress(loss,value, max=100):\n",
        "    return HTML(\"\"\" Batch loss :{loss}\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(loss=loss,value=value, max=max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbzcZh4-QoOG"
      },
      "outputs": [],
      "source": [
        "num_of_epochs=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXC08R9hQq13"
      },
      "outputs": [],
      "source": [
        "#Sets the module in training mode\n",
        "model.train()\n",
        "\n",
        "loss_per_10_steps=[]\n",
        "for epoch in range(1,num_of_epochs+1):\n",
        "  print('Running epoch: {}'.format(epoch))\n",
        "  \n",
        "  running_loss=0\n",
        "\n",
        "  out = display(progress(1, num_of_batches+1), display_id=True)\n",
        "  for i in range(num_of_batches):\n",
        "    inputbatch=[]\n",
        "    labelbatch=[]\n",
        "    new_df=train_df[i*batch_size:i*batch_size+batch_size]\n",
        "    for indx,row in new_df.iterrows():\n",
        "    #   print(indx, row)\n",
        "        val1 = row['ACTOR1']\n",
        "        val2 = row['ACTOR2']\n",
        "        if pd.isnull(val1) and pd.isnull(val2):\n",
        "            input = 'TEXT: '+ str(row['EVENT_DATE']) + ' | '+ row['SOURCE'] + ' | '+ str(row['FATALITIES']) + ' | '+ row['EVENT_TYPE']+ ' | '+ row['SUB_EVENT_TYPE']+ ' | '+ row['LOCATION']\n",
        "        elif pd.isnull(val1):\n",
        "            input = 'TEXT: '+ str(row['EVENT_DATE']) + ' | '+ row['SOURCE'] + ' | '+ str(row['FATALITIES']) + ' | '+ row['EVENT_TYPE']+ ' | '+ row['SUB_EVENT_TYPE']+ ' | '+ row['LOCATION'] + ' | '+ str(row['ACTOR2'])\n",
        "        else:\n",
        "            input = 'TEXT: '+ str(row['EVENT_DATE']) + ' | '+ row['SOURCE'] + ' | '+ str(row['FATALITIES']) + ' | '+ row['EVENT_TYPE']+ ' | '+ row['SUB_EVENT_TYPE']+ ' | '+ row['LOCATION'] + ' | '+ str(row['ACTOR1'])\n",
        "    \n",
        "        # input = 'TEXT: '+ str(row['EVENT_DATE'])+ row['SOURCE'] + str(row['FATALITIES']) + row['EVENT_TYPE']+ row['SUB_EVENT_TYPE']+ row['LOCATION'] + str(row['ACTOR1'])+ str(row['ACTOR2'])+'</s>' \n",
        "    \n",
        "        labels = row['NOTES']+'</s>'   \n",
        "        inputbatch.append(input)\n",
        "        labelbatch.append(labels)\n",
        "    inputbatch=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=400,return_tensors='pt')[\"input_ids\"]\n",
        "    labelbatch=tokenizer.batch_encode_plus(labelbatch,padding=True,max_length=400,return_tensors=\"pt\") [\"input_ids\"]\n",
        "    inputbatch=inputbatch.to(dev)\n",
        "    labelbatch=labelbatch.to(dev)\n",
        "\n",
        "    # clear out the gradients of all Variables \n",
        "    optimizer.zero_grad()\n",
        "    torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "    # Forward propogation\n",
        "    outputs = model(input_ids=inputbatch, labels=labelbatch)\n",
        "    loss = outputs.loss\n",
        "    loss_num=loss.item()\n",
        "    logits = outputs.logits\n",
        "    running_loss+=loss_num\n",
        "    if i%10 ==0:      \n",
        "      loss_per_10_steps.append(loss_num)\n",
        "    out.update(progress(loss_num,i, num_of_batches+1))\n",
        "\n",
        "    # calculating the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #updating the params\n",
        "    optimizer.step()\n",
        "    \n",
        "  running_loss=running_loss/int(num_of_batches)\n",
        "  print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(valid_df)\n",
        "final_valid = []\n",
        "for indx,row in valid_df.iterrows():\n",
        "    val1 = row['ACTOR1']\n",
        "    val2 = row['ACTOR2']\n",
        "    if pd.isnull(val1) and pd.isnull(val2):\n",
        "        text = str(row['EVENT_DATE']) + ' | '+ row['SOURCE'] + ' | '+ str(row['FATALITIES']) + ' | '+ row['EVENT_TYPE']+ ' | '+ row['SUB_EVENT_TYPE']+ ' | '+ row['LOCATION']\n",
        "    elif pd.isnull(val1):\n",
        "        text = str(row['EVENT_DATE']) + ' | '+ row['SOURCE'] + ' | '+ str(row['FATALITIES']) + ' | '+ row['EVENT_TYPE']+ ' | '+ row['SUB_EVENT_TYPE']+ ' | '+ row['LOCATION'] + ' | '+ str(row['ACTOR2'])\n",
        "    else:\n",
        "        text = str(row['EVENT_DATE']) + ' | '+ row['SOURCE'] + ' | '+ str(row['FATALITIES']) + ' | '+ row['EVENT_TYPE']+ ' | '+ row['SUB_EVENT_TYPE']+ ' | '+ row['LOCATION'] + ' | '+ str(row['ACTOR1'])\n",
        "    # text = str(row['EVENT_DATE']) + ' | '+ row['SOURCE'] + ' | '+ str(row['FATALITIES']) + ' | '+ row['EVENT_TYPE']+ ' | '+ row['SUB_EVENT_TYPE']+ ' | '+ row['LOCATION'] + ' | '+ str(row['ACTOR1'])+ ' | '+ str(row['ACTOR2'])\n",
        "    \n",
        "    final_valid.append(text)"
      ],
      "metadata": {
        "id": "szPvkATydXGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_valid)"
      ],
      "metadata": {
        "id": "sqepO4NrIU3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "final_valid_text = []\n",
        "encodings = []\n",
        "for text in final_valid:\n",
        "    input_ids = tokenizer.encode(\"TEXT: {} </s>\".format(text), return_tensors=\"pt\")  # Batch size 1\n",
        "    input_ids=input_ids.to(dev)\n",
        "    # encodings.append(input_ids)\n",
        "    outputs = model.generate(input_ids)\n",
        "    final_txt = tokenizer.decode(outputs[0]).replace('<pad>','').replace('</s>','')\n",
        "    final_valid_text.append(final_txt)"
      ],
      "metadata": {
        "id": "nAsL0U6NdfC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_valid_text)"
      ],
      "metadata": {
        "id": "xe6fesCQxrWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "max_length = model.config.n_positions\n",
        "stride = 512\n",
        "\n",
        "nlls = []\n",
        "for i in tqdm(range(0, input_ids.size(1), stride)):\n",
        "    begin_loc = max(i + stride - max_length, 0)\n",
        "    end_loc = min(i + stride, input_ids.size(1))\n",
        "    trg_len = end_loc - i  # may be different from stride on last loop\n",
        "    input_ids = input_ids[:, begin_loc:end_loc].to(dev)\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        neg_log_likelihood = outputs[0] * trg_len\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
      ],
      "metadata": {
        "id": "BAfeIPqRLPOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppl"
      ],
      "metadata": {
        "id": "2w6OM_TLLwvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02RHVYNoTp00"
      },
      "outputs": [],
      "source": [
        "! pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()"
      ],
      "metadata": {
        "id": "IrbBpHNfWaFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = []\n",
        "for txt in valid_df['NOTES']:\n",
        "    reference.append(txt)\n"
      ],
      "metadata": {
        "id": "dEt4z8VTW3ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_valid_text)"
      ],
      "metadata": {
        "id": "FPOoeuNwIG5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(reference)"
      ],
      "metadata": {
        "id": "-Fgd9kx2XW4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pUEdTzNbXWJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge.get_scores(final_valid_text, reference, avg = True)"
      ],
      "metadata": {
        "id": "0qeBiwfeWnYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jWEQ7tdzGvRx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "data_to_text_task2_large_milestone3.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}