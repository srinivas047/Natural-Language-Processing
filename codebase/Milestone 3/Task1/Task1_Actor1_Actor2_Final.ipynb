{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task1_Actor1_Actor2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import re"
      ],
      "metadata": {
        "id": "eKNtIx5mXDVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/task_1_information_extraction_train.tsv', sep = '\\t')\n",
        "print(len(data))"
      ],
      "metadata": {
        "id": "xESRkRQQYITV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removies Null rows and reseting Index\n",
        "data = data[data['NOTES'].notna()]\n",
        "data = data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "nnH_dnjiEWi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACTOR 1, Data Processing\n",
        "zero_numbering_event = {}\n",
        "for i in range(len(data['ACTOR1'].unique())):\n",
        "    cls = data['ACTOR1'].unique()[i]\n",
        "    zero_numbering_event[cls] = i\n",
        "\n",
        "data['ACTOR1'] = data['ACTOR1'].apply(lambda x: zero_numbering_event[x])"
      ],
      "metadata": {
        "id": "3ZYtIXLcIwrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACTOR 2, Data Processing\n",
        "zero_numbering_event_sub = {}\n",
        "for i in range(len(data['ACTOR2'].unique())):\n",
        "    cls = data['ACTOR2'].unique()[i]\n",
        "    zero_numbering_event_sub[cls] = i\n",
        "\n",
        "data['ACTOR2'] = data['ACTOR2'].apply(lambda x: zero_numbering_event_sub[x])"
      ],
      "metadata": {
        "id": "vwk9OlXuIHBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = spacy.load('en')\n",
        "def tokenize (text):\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
        "    nopunct = regex.sub(\" \", text.lower())\n",
        "    return [token.text for token in tok.tokenizer(nopunct)]"
      ],
      "metadata": {
        "id": "iAzD3eS8YPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = Counter()\n",
        "docs = {}\n",
        "for i in range(len(data['NOTES'])):\n",
        "    docs[i] = tokenize(data['NOTES'][i])\n",
        "    counts.update(tokenize(data['NOTES'][i]))"
      ],
      "metadata": {
        "id": "-xwlVkbtYLFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating vocabulary\n",
        "vocab2index = {\"\":0, \"UNK\":1}\n",
        "words = [\"\", \"UNK\"]\n",
        "for word in counts:\n",
        "    vocab2index[word] = len(words)\n",
        "    words.append(word)"
      ],
      "metadata": {
        "id": "XmCuD8J9YmPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(text, vocab2index, N=70):\n",
        "    tokenized = tokenize(text)\n",
        "    encoded = np.zeros(N, dtype=int)\n",
        "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
        "    length = min(N, len(enc1))\n",
        "    encoded[:length] = enc1[:length]\n",
        "    return encoded, length"
      ],
      "metadata": {
        "id": "Xam2bg7BYqrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['encoded'] = data['NOTES'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))"
      ],
      "metadata": {
        "id": "frT48JCZYtbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
      ],
      "metadata": {
        "id": "vkVNYpHRZKq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = list(data['encoded'])\n",
        "y_event = list(data['ACTOR1'])\n",
        "y_sub_event = list(data['ACTOR2'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_event, X_valid_event, y_train_event, y_valid_event = train_test_split(X, y_event, test_size=0.3)\n",
        "\n",
        "X_train_sub_event, X_valid_sub_event, y_train_sub_event, y_valid_sub_event = train_test_split(X, y_sub_event, test_size=0.3)"
      ],
      "metadata": {
        "id": "h8gVoWGEZPcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_event = ReviewsDataset(X_train_event, y_train_event)\n",
        "valid_ds_event = ReviewsDataset(X_valid_event, y_valid_event)\n",
        "\n",
        "train_ds_sub_event = ReviewsDataset(X_train_sub_event, y_train_sub_event)\n",
        "valid_ds_sub_event = ReviewsDataset(X_valid_sub_event, y_valid_sub_event)"
      ],
      "metadata": {
        "id": "xIa__Rs_cJR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_dl,val_dl, model, epochs=10, lr=0.001):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        for x, y, l in train_dl:\n",
        "            x = x.long()\n",
        "            y = y.long()\n",
        "            y_pred = model(x, l)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
        "        if i % 2 == 0:\n",
        "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
        "\n",
        "def validation_metrics (model, valid_dl):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    sum_rmse = 0.0\n",
        "    for x, y, l in valid_dl:\n",
        "        x = x.long()\n",
        "        y = y.long()\n",
        "        y_hat = model(x, l)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        pred = torch.max(y_hat, 1)[1]\n",
        "        correct += (pred == y).float().sum()\n",
        "        total += y.shape[0]\n",
        "        sum_loss += loss.item()*y.shape[0]\n",
        "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
        "    return sum_loss/total, correct/total, sum_rmse/total"
      ],
      "metadata": {
        "id": "NNnRN2B7cMhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_fixed_len(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, classes) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional = True)\n",
        "        self.linear = nn.Linear(hidden_dim, classes)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "metadata": {
        "id": "2D8DWkjvcV6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5000\n",
        "vocab_size = len(words)\n",
        "\n",
        "train_dl_event = DataLoader(train_ds_event, batch_size=batch_size, shuffle=True)\n",
        "val_dl_event = DataLoader(valid_ds_event, batch_size=batch_size)\n",
        "\n",
        "train_dl_sub_event = DataLoader(train_ds_sub_event, batch_size=batch_size, shuffle=True)\n",
        "val_dl_sub_event = DataLoader(valid_ds_sub_event, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "bCg2mfqfcm63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(data['ACTOR1'].unique()))\n",
        "print(len(data['ACTOR2'].unique()))\n"
      ],
      "metadata": {
        "id": "51pGXiUhU2Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_actor1 =  LSTM_fixed_len(vocab_size, 50, 50, 3384)\n",
        "model_actor2 =  LSTM_fixed_len(vocab_size, 50, 50, 2826)"
      ],
      "metadata": {
        "id": "7-O7AV3dcRl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_dl_event,val_dl_event, model_actor1, epochs=100, lr=0.1)"
      ],
      "metadata": {
        "id": "2puozr0Qc0uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_dl_sub_event,val_dl_sub_event, model_actor2, epochs=100, lr=0.1)"
      ],
      "metadata": {
        "id": "M0Jn5w76TZEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5000\n",
        "vocab_size = len(words)\n",
        "\n",
        "model_actor1_test =  LSTM_fixed_len(vocab_size, 50, 50, 3384)\n",
        "model_actor2_test =  LSTM_fixed_len(vocab_size, 50, 50, 2826)"
      ],
      "metadata": {
        "id": "qatWoFvv_fcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata = pd.read_csv('/content/task_1_information_extraction_valid.tsv', sep = '\\t')\n",
        "# testdata = testdata\n",
        "\n",
        "# testdata['NOTES'] = testdata['NOTES'].fillna('')\n",
        "testdata = testdata[testdata['NOTES'].notna()]\n",
        "testdata = testdata.reset_index(drop=True)\n",
        "testdata['encoded'] = testdata['NOTES'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n"
      ],
      "metadata": {
        "id": "UIQ9BH-qdS93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata.head()"
      ],
      "metadata": {
        "id": "Jxo-s5uvbTx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excerpts_test = testdata['encoded']\n",
        "X_test = [i[0] for i in excerpts_test]\n",
        "l_test = [i[1] for i in excerpts_test]\n",
        "X_test = torch.LongTensor(X_test)"
      ],
      "metadata": {
        "id": "qrgc58yOdtmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_actor1_test = model_actor1_test(X_test, l_test)\n",
        "\n",
        "y_actor2_test = model_actor2_test(X_test, l_test)\n"
      ],
      "metadata": {
        "id": "k_eWgcFXdm0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Final(y_hat):\n",
        "    Final = []\n",
        "    for i in y_hat:\n",
        "        Final.append(torch.argmax(i).item())\n",
        "    return Final"
      ],
      "metadata": {
        "id": "d5aOeRgcd64F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Final_Actors1_test = Final(y_actor1_test)\n",
        "Final_Actors2_test = Final(y_actor2_test)"
      ],
      "metadata": {
        "id": "wgdPspVDf7-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted Events test : \", Counter(Final_Actors1_test))\n",
        "\n",
        "print(\"Predicted Sub Events test : \", Counter(Final_Actors2_test))"
      ],
      "metadata": {
        "id": "PBPSC6Q3iMKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACTOR 1, Data Processing\n",
        "zero_numbering_event1 = {}\n",
        "for i in range(len(testdata['ACTOR1'].unique())):\n",
        "    cls = testdata['ACTOR1'].unique()[i]\n",
        "    zero_numbering_event1[cls] = i\n",
        "\n",
        "testdata['ACTOR1'] = testdata['ACTOR1'].apply(lambda x: zero_numbering_event1[x])\n",
        "\n",
        "# ACTOR 2, Data Processing\n",
        "zero_numbering_event2 = {}\n",
        "for i in range(len(testdata['ACTOR2'].unique())):\n",
        "    cls = testdata['ACTOR2'].unique()[i]\n",
        "    zero_numbering_event2[cls] = i\n",
        "\n",
        "testdata['ACTOR2'] = testdata['ACTOR2'].apply(lambda x: zero_numbering_event2[x])"
      ],
      "metadata": {
        "id": "r-VyCuE0yMhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Actual Actor1 : \", Counter(testdata['ACTOR1']))\n",
        "print(\"Actual Actor2 : \", Counter(testdata['ACTOR2']))"
      ],
      "metadata": {
        "id": "-0TBuNntX4y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(\"Actor1 F1 Score :\", f1_score(testdata['ACTOR1'], Final_Actors1_test, average='macro'))\n",
        "print(\"Actor2 F1 Score :\", f1_score(testdata['ACTOR2'], Final_Actors2_test, average='macro'))"
      ],
      "metadata": {
        "id": "B9rDotXRjvpx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}