{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eKNtIx5mXDVo"},"outputs":[],"source":["#library imports\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import re\n","import spacy\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import string\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from sklearn.metrics import mean_squared_error\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xESRkRQQYITV"},"outputs":[],"source":["import pandas as pd\n","data = pd.read_csv('/content/task_1_information_extraction_train.tsv', sep = '\\t')\n","print(len(data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnH_dnjiEWi6"},"outputs":[],"source":["# Removies Null rows and reseting Index\n","data = data[data['NOTES'].notna()]\n","data = data.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZYtIXLcIwrO"},"outputs":[],"source":["# Event Type, Data Processing\n","zero_numbering_event = {}\n","for i in range(len(data['EVENT_TYPE'].unique())):\n","    cls = data['EVENT_TYPE'].unique()[i]\n","    zero_numbering_event[cls] = i\n","\n","data['EVENT'] = data['EVENT_TYPE'].apply(lambda x: zero_numbering_event[x])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwk9OlXuIHBk"},"outputs":[],"source":["# Sub Event Type, Data Processing\n","zero_numbering_event_sub = {}\n","for i in range(len(data['SUB_EVENT_TYPE'].unique())):\n","    cls = data['SUB_EVENT_TYPE'].unique()[i]\n","    zero_numbering_event_sub[cls] = i\n","\n","data['SUB_EVENT'] = data['SUB_EVENT_TYPE'].apply(lambda x: zero_numbering_event_sub[x])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAzD3eS8YPHZ"},"outputs":[],"source":["tok = spacy.load('en')\n","def tokenize (text):\n","    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n","    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n","    nopunct = regex.sub(\" \", text.lower())\n","    return [token.text for token in tok.tokenizer(nopunct)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xwlVkbtYLFf"},"outputs":[],"source":["counts = Counter()\n","docs = {}\n","for i in range(len(data['NOTES'])):\n","    docs[i] = tokenize(data['NOTES'][i])\n","    counts.update(tokenize(data['NOTES'][i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmCuD8J9YmPG"},"outputs":[],"source":["#creating vocabulary\n","vocab2index = {\"\":0, \"UNK\":1}\n","words = [\"\", \"UNK\"]\n","for word in counts:\n","    vocab2index[word] = len(words)\n","    words.append(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xam2bg7BYqrw"},"outputs":[],"source":["def encode_sentence(text, vocab2index, N=70):\n","    tokenized = tokenize(text)\n","    encoded = np.zeros(N, dtype=int)\n","    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n","    length = min(N, len(enc1))\n","    encoded[:length] = enc1[:length]\n","    return encoded, length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"frT48JCZYtbA"},"outputs":[],"source":["data['encoded'] = data['NOTES'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkVNYpHRZKq6"},"outputs":[],"source":["class ReviewsDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.y = Y\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8gVoWGEZPcV"},"outputs":[],"source":["X = list(data['encoded'])\n","y_event = list(data['EVENT'])\n","y_sub_event = list(data['SUB_EVENT'])\n","from sklearn.model_selection import train_test_split\n","X_train_event, X_valid_event, y_train_event, y_valid_event = train_test_split(X, y_event, test_size=0.3)\n","\n","X_train_sub_event, X_valid_sub_event, y_train_sub_event, y_valid_sub_event = train_test_split(X, y_sub_event, test_size=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIa__Rs_cJR0"},"outputs":[],"source":["train_ds_event = ReviewsDataset(X_train_event, y_train_event)\n","valid_ds_event = ReviewsDataset(X_valid_event, y_valid_event)\n","\n","train_ds_sub_event = ReviewsDataset(X_train_sub_event, y_train_sub_event)\n","valid_ds_sub_event = ReviewsDataset(X_valid_sub_event, y_valid_sub_event)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNnRN2B7cMhJ"},"outputs":[],"source":["def train_model(train_dl,val_dl, model, epochs=10, lr=0.001):\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optimizer = torch.optim.Adam(parameters, lr=lr)\n","    for i in range(epochs):\n","        model.train()\n","        sum_loss = 0.0\n","        total = 0\n","        for x, y, l in train_dl:\n","            x = x.long()\n","            y = y.long()\n","            y_pred = model(x, l)\n","            optimizer.zero_grad()\n","            loss = F.cross_entropy(y_pred, y)\n","            loss.backward()\n","            optimizer.step()\n","            sum_loss += loss.item()*y.shape[0]\n","            total += y.shape[0]\n","        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n","        if i % 2 == 0:\n","            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n","\n","def validation_metrics (model, valid_dl):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    sum_loss = 0.0\n","    sum_rmse = 0.0\n","    for x, y, l in valid_dl:\n","        x = x.long()\n","        y = y.long()\n","        y_hat = model(x, l)\n","        loss = F.cross_entropy(y_hat, y)\n","        pred = torch.max(y_hat, 1)[1]\n","        correct += (pred == y).float().sum()\n","        total += y.shape[0]\n","        sum_loss += loss.item()*y.shape[0]\n","        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n","    return sum_loss/total, correct/total, sum_rmse/total"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2D8DWkjvcV6e"},"outputs":[],"source":["class LSTM_fixed_len(torch.nn.Module) :\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, classes) :\n","        super().__init__()\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional = True)\n","        self.linear = nn.Linear(hidden_dim, classes)\n","        self.dropout = nn.Dropout(0.2)\n","        \n","    def forward(self, x, l):\n","        x = self.embeddings(x)\n","        x = self.dropout(x)\n","        lstm_out, (ht, ct) = self.lstm(x)\n","        return self.linear(ht[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCg2mfqfcm63"},"outputs":[],"source":["batch_size = 5000\n","vocab_size = len(words)\n","\n","train_dl_event = DataLoader(train_ds_event, batch_size=batch_size, shuffle=True)\n","val_dl_event = DataLoader(valid_ds_event, batch_size=batch_size)\n","\n","train_dl_sub_event = DataLoader(train_ds_sub_event, batch_size=batch_size, shuffle=True)\n","val_dl_sub_event = DataLoader(valid_ds_sub_event, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51pGXiUhU2Nm"},"outputs":[],"source":["\n","print(len(data['EVENT'].unique()))\n","print(len(data['SUB_EVENT'].unique()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-O7AV3dcRl8"},"outputs":[],"source":["model_event =  LSTM_fixed_len(vocab_size, 50, 50, 6)\n","model_sub_event =  LSTM_fixed_len(vocab_size, 50, 50, 25)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2puozr0Qc0uu"},"outputs":[],"source":["train_model(train_dl_event,val_dl_event, model_event, epochs=50, lr=0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIS5wcTuFEg-"},"outputs":[],"source":["path = F\"/content/gdrive/My Drive/Event.pt\"\n","torch.save(model_event.state_dict(), path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0Jn5w76TZEH"},"outputs":[],"source":["train_model(train_dl_sub_event,val_dl_sub_event, model_sub_event, epochs=50, lr=0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQf2eQ8OFF31"},"outputs":[],"source":["path = F\"/content/gdrive/My Drive/Sub_Event.pt\"\n","torch.save(model_sub_event.state_dict(), path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EooV7_Ao_RC3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qatWoFvv_fcM"},"outputs":[],"source":["batch_size = 5000\n","vocab_size = len(words)\n","\n","model_event_test =  LSTM_fixed_len(vocab_size, 50, 50, 6)\n","model_sub_event_test =  LSTM_fixed_len(vocab_size, 50, 50, 25)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIQ9BH-qdS93"},"outputs":[],"source":["testdata = pd.read_csv('/content/task_1_information_extraction_valid.tsv', sep = '\\t')\n","# testdata = testdata\n","\n","# testdata['NOTES'] = testdata['NOTES'].fillna('')\n","testdata = testdata[testdata['NOTES'].notna()]\n","testdata = testdata.reset_index(drop=True)\n","testdata['encoded'] = testdata['NOTES'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jxo-s5uvbTx5"},"outputs":[],"source":["testdata.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrgc58yOdtmA"},"outputs":[],"source":["excerpts_test = testdata['encoded']\n","X_test = [i[0] for i in excerpts_test]\n","l_test = [i[1] for i in excerpts_test]\n","X_test = torch.LongTensor(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_eWgcFXdm0C"},"outputs":[],"source":["y_event_test = model_event_test(X_test, l_test)\n","\n","y_sub_event_test = model_sub_event_test(X_test, l_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5aOeRgcd64F"},"outputs":[],"source":["def Final(y_hat):\n","    Final = []\n","    for i in y_hat:\n","        Final.append(torch.argmax(i).item())\n","    return Final"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgdPspVDf7-h"},"outputs":[],"source":["Final_Events_test = Final(y_event_test)\n","Final_Sub_Events_test = Final(y_sub_event_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBPSC6Q3iMKY"},"outputs":[],"source":["print(\"Predicted Events test : \", Counter(Final_Events_test))\n","\n","print(\"Predicted Sub Events test : \", Counter(Final_Sub_Events_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqSVTM1akmrd"},"outputs":[],"source":["#changing ratings to 0-numbering\n","testdata['EVENT'] = testdata['EVENT_TYPE'].apply(lambda x: zero_numbering_event[x])\n","testdata['SUB_EVENT'] = testdata['SUB_EVENT_TYPE'].apply(lambda x: zero_numbering_event_sub[x])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0TBuNntX4y6"},"outputs":[],"source":["print(\"Actual Events : \", Counter(testdata['EVENT']))\n","print(\"Actual Sub Events : \", Counter(testdata['SUB_EVENT']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9rDotXRjvpx"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","print(\"Events F1 Score :\", f1_score(testdata['EVENT'], Final_Events_test, average='macro'))\n","print(\"Sub Events test F1 Score :\", f1_score(testdata['SUB_EVENT'], Final_Sub_Events_test, average='macro'))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Task1_Event_SubEvent.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
